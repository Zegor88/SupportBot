# Epic E4: Answer Generation - Task Decomposition

## 1. Обзор и цели

**Эпик:** E4. Answer Generation
**Цель:** Сформировать связный и релевантный ответ на запрос пользователя с использованием LLM, основываясь на системных инструкциях, истории диалога и данных из базы знаний (RAG).
**Критерий выполнения (Done-критерий из PRD):** ≥ 50 % кейсов закрыты ботом.
**Ключевые компоненты:** `AnswerAgent`, `OpenAI API (GPT-4o-mini)`, `dynamic_instructions`, `PromptManager`.

---

## 2. Декомпозиция задач

Ниже представлен список задач для реализации эпика, упорядоченный по приоритету выполнения.

**Примечание:** Реализация полноценного извлечения истории диалога и контекста из RAG-базы относится к **Эпику E5**. В рамках текущего эпика мы создаем для них заглушки и `TODO`, чтобы архитектура была готова к их будущей интеграции.

### Задача 1 [ЗАВЕРШЕНО]: Создание структуры `AnswerAgent`

- **Описание:** Разработать базовую структуру для `AnswerAgent`. Согласно документации OpenAI Agent SDK, агент будет сконфигурирован с динамической функцией для генерации инструкций (`dynamic_instructions`), которая и будет формировать итоговый промпт.
- **Приоритет:** **Высокий**
- **Зависимости:** Архитектура на `OpenAI Agent SDK` (Эпик E0).
- **Предлагаемое решение:**
    1. Создать файл `src/agents/answer_agent.py`.
    2. Внутри файла определить функцию `build_answer_prompt` (см. Задачу 2), которая будет отвечать за сборку промпта.
    3. Определить и экспортировать экземпляр агента: `answer_agent = Agent(...)`.
    4. Сконфигурировать агент, передав ему `name`, `model` и, самое главное, `instructions=build_answer_prompt`. Это соответствует подходу `dynamic_instructions` из документации SDK.
    5. Определить Pydantic-модель `ReplyHandoffData` в `src/models/handoff.py` для стандартизации данных, передаваемых агенту от `RouterAgent`. Она будет передаваться через `RunContext`.

---

### Задача 2 [ЗАВЕРШЕНО]: Реализация функции для динамической сборки промпта

- **Описание:** Создать функцию, которая будет динамически собирать промпт для LLM. Эта функция будет использоваться в качестве `dynamic_instructions` для `AnswerAgent`.
- **Приоритет:** **Высокий**
- **Зависимости:** Задача 1.
- **Предлагаемое решение:**
    1. Создать новый модуль `src/prompts/builders.py`.
    2. Реализовать в нем асинхронную функцию `build_answer_prompt(context: RunContextWrapper[UserContext], agent: Agent[UserContext]) -> str:`.
    3. Внутри функции:
        - Получить `ReplyHandoffData` из `context`.
        - Загрузить системный промпт по `system_prompt_key` из хранилища (например, `prompts.yaml`).
        - **Добавить заглушки для будущих данных:**
            - `// TODO: [E5] Заменить заглушку на реальное получение и форматирование истории диалога.`
            - `history_str = "Conversation history is not yet implemented."`
            - `// TODO: [E5] Заменить заглушку на реальный контекст, полученный от RetrieverAgent.`
            - `rag_context_str = handoff_data.context or "No additional context provided."`
        - Собрать итоговый промпт из всех частей: системной инструкции, заглушек для истории и контекста, и сообщения пользователя.
    4. Функция должна возвращать промпт в формате, который ожидает `Agent` (строку или список словарей).

---

### Задача 3 [ЗАВЕРШЕНО]: Интеграция с OpenAI API для генерации ответа

- **Описание:** Убедиться, что `AnswerAgent` корректно вызывает модель `GPT-4o-mini` с собранным промптом для получения ответа.
- **Приоритет:** **Высокий**
- **Зависимости:** Задача 2.
- **Предлагаемое решение:**
    1. Эта задача в основном решается конфигурацией `Agent` в Задаче 1. Нужно правильно указать `model` и `model_settings`.
    2. Задать параметры вызова в `model_settings`:
        - `model="gpt-4o-mini"`
        - `max_tokens=500` (согласно PRD)
        - `temperature=0.7` (рекомендуемое стартовое значение)
    3. SDK автоматически обработает вызов API, используя промпт из `build_answer_prompt`.
    4. Настроить обработку ошибок API (например, `RateLimitError`) можно через `lifecycle hooks` агента или на уровне `Runner`, если SDK это поддерживает.

---

### Задача 4 [ЗАВЕРШЕНО]: Постобработка и форматирование ответа

- **Описание:** Подготовить сгенерированный LLM ответ для отправки в Telegram.
- **Приоритет:** **Средний**
- **Зависимости:** Задача 3.
- **Предлагаемое решение:**
    1. Создать утилитарную функцию `format_for_telegram(text: str) -> str` в `src/bot/utils.py`.
    2. Функция должна очищать ответ от артефактов, которые может сгенерировать модель (например, префиксы "Ответ:", "Assistant:").
    3. Экранировать специальные символы для `MarkdownV2`, чтобы избежать ошибок при отправке.

---

### Задача 5 [ЗАВЕРШЕНО]: Интеграция с `LoggerAgent`

- **Описание:** Обеспечить запись всех данных, связанных с генерацией ответа, в базу данных.
- **Приоритет:** **Средний**
- **Зависимости:** Задача 3, Эпик E6 (Logging).
- **Предлагаемое решение:**
    1. После успешного выполнения `AnswerAgent` основной обработчик (`handler`) должен вызывать `LoggerAgent`.
    2. В лог должны записываться все компоненты из **FR-13**: `{ts, uid, rule_id, action, q, a, ctx}`.
    3. **Важно:** Поле `ctx` будет временно содержать заглушку или пустую строку до завершения Эпика E5. Это нужно отметить в коде логирования.